{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data Science\\\\NLP\\\\SentimentAnalysis\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data Science\\\\NLP\\\\SentimentAnalysis'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import yaml\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.\tTo be updated entity > constructor file (__init__.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EDAdataCleanerConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.\tTo be updated config > configuration.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SentimentAnalysis.constants import *\n",
    "from SentimentAnalysis.utils.common import read_yaml\n",
    "from SentimentAnalysis.utils.common import create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    \n",
    "    def __init__(\n",
    "                self,\n",
    "                config_file_path = CONFIG_FILE_PATH,\n",
    "                params_file_path = PARAMS_FILE_PATH,\n",
    "                schema_filepath = SCHEMA_FILE_PATH):\n",
    "            \n",
    "\n",
    "            self.config = read_yaml(config_file_path)\n",
    "            self.params = read_yaml(params_file_path)\n",
    "            self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "            create_directories([self.config.dataStore_root])\n",
    "\n",
    "    def get_data_clean_config(self)-> EDAdataCleanerConfig:\n",
    "          config = self.config.EDA_DataCleaner\n",
    "\n",
    "          create_directories([config.root_dir])\n",
    "          \n",
    "          data_cleaner_config = EDAdataCleanerConfig(\n",
    "                root_dir=config.root_dir,\n",
    "                data_path=config.data_path,\n",
    "                 \n",
    "          )     \n",
    "          return data_cleaner_config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from SentimentAnalysis.logging import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shanusingh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shanusingh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shanusingh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pandas as pd\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "##Setup the English stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "class DataCleaner:\n",
    "    def __init__(self, config: EDAdataCleanerConfig):\n",
    "        self.config = config\n",
    "        self.tokenizer = ToktokTokenizer()\n",
    "        self.stopwords_list = set(stopwords.words('english'))\n",
    "        self.stemmer = SnowballStemmer(language='english')\n",
    "        self.lb = LabelBinarizer()\n",
    "\n",
    "    def html(self, text):\n",
    "        # Remove HTML tags from the text\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        return soup.get_text()\n",
    "\n",
    "    def deEmojify(self, text):\n",
    "        # Remove emojis from the text\n",
    "        regrex_pattern = re.compile(pattern=\"[\"\n",
    "                                      u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                      u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                      u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                      u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                      \"]+\", flags=re.UNICODE)\n",
    "        return regrex_pattern.sub(r'', text)\n",
    "\n",
    "    def to_unicode(self, text):\n",
    "        # Convert text to unicode\n",
    "        if isinstance(text, float):\n",
    "            text = str(text)\n",
    "        if isinstance(text, int):\n",
    "            text = str(text)\n",
    "        if not isinstance(text, str):\n",
    "            text = text.decode('utf-8', 'ignore')\n",
    "        return text\n",
    "\n",
    "    def remove_between_square_brackets(self, text):\n",
    "        # Remove text between square brackets\n",
    "        return re.sub(r'\\[[^]]*\\]', '', text)\n",
    "\n",
    "    def remove_special_characters(self, text, remove_digits=True):\n",
    "        # Remove special characters and optionally digits\n",
    "        pattern = r'[^a-zA-z0-9\\s]' if remove_digits else r'[^a-zA-z\\s]'\n",
    "        text = re.sub(pattern, '', text)\n",
    "        return text\n",
    "\n",
    "    def denoise_text(self, text):\n",
    "        # Denoise the text using multiple cleaning steps\n",
    "        text = self.to_unicode(text)\n",
    "        text = self.html(text)\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        text = self.deEmojify(text)\n",
    "        #text = text.encode('ascii', 'ignore')\n",
    "        #text = text.to_unicode(text)\n",
    "        text = self.remove_between_square_brackets(text)\n",
    "        text = self.remove_special_characters(text)\n",
    "        text = text.lower()\n",
    "        return text\n",
    "\n",
    "    def remove_stopwords(self, text, is_lower_case=False):\n",
    "        # Remove stopwords from the text\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        tokens = [token.strip() for token in tokens]\n",
    "        if is_lower_case:\n",
    "            filtered_tokens = [token for token in tokens if token not in self.stopwords_list]\n",
    "        else:\n",
    "            filtered_tokens = [token for token in tokens if token.lower() not in self.stopwords_list]\n",
    "        filtered_text = ' '.join(filtered_tokens)\n",
    "        return filtered_text\n",
    "\n",
    "    def apply_stemming(self, review):\n",
    "        # Apply stemming to the text\n",
    "        return ' '.join([self.stemmer.stem(word) for word in self.tokenizer.tokenize(review)])\n",
    "\n",
    "    \n",
    "    def binarize_sentiment(self, sentiment):\n",
    "        # Binarize sentiment labels (Positive: 1, Negative: 0)\n",
    "        return self.lb.fit_transform(sentiment)\n",
    "\n",
    "    def convert_examples_to_features(self):\n",
    "        data = pd.read_csv(self.config.data_path)\n",
    "        # Assuming example_batch is a DataFrame with 'review' and 'sentiment' columns\n",
    "        data['review'] = data['review'].apply(self.denoise_text)\n",
    "        data['review'] = data['review'].apply(self.remove_stopwords)\n",
    "        data['review'] = data['review'].apply(self.apply_stemming)\n",
    "        data['sentiment'] = self.binarize_sentiment(data['sentiment'])\n",
    "\n",
    "        \n",
    "        data.to_csv(os.path.join(self.config.root_dir, \"data.csv\"), index=False)\n",
    "        logger.info(data.shape)\n",
    "        print(data.shape)\n",
    "        logger.info(data.head)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-12 09:32:00,134: INFO: common: YAML file loaded successfully: config\\config.yaml]\n",
      "[2023-08-12 09:32:00,134: INFO: common: YAML file loaded successfully: params.yaml]\n",
      "[2023-08-12 09:32:00,142: INFO: common: YAML file loaded successfully: schema.yaml]\n",
      "[2023-08-12 09:32:00,142: INFO: common: Created directory at: dataStore]\n",
      "[2023-08-12 09:32:00,142: INFO: common: Created directory at: dataStore/EDA_DataCleaner]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanusingh\\AppData\\Local\\Temp\\ipykernel_24944\\1480734748.py:29: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-12 09:33:10,276: INFO: 1480734748: (30000, 2)]\n",
      "(30000, 2)\n",
      "[2023-08-12 09:33:10,276: INFO: 1480734748: <bound method NDFrame.head of                                                   review  sentiment\n",
      "0      one review mention watch 1 oz episod youll hoo...          1\n",
      "1      wonder littl product film techniqu unassum old...          1\n",
      "2      thought wonder way spend time hot summer weeke...          1\n",
      "3      basic there famili littl boy jake think there ...          0\n",
      "4      petter mattei love time money visual stun film...          1\n",
      "...                                                  ...        ...\n",
      "29995  new york love final make shore 10 short stori ...          1\n",
      "29996  movi make wish imdb would let vote zero one tw...          0\n",
      "29997  space camp unfortun luck plan around time chal...          0\n",
      "29998  octavio paz mexican poet writer diplomat recei...          1\n",
      "29999  watch 10 minut movi bewild watch 30 minut toe ...          0\n",
      "\n",
      "[30000 rows x 2 columns]>]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_cleaner_config = config.get_data_clean_config()\n",
    "    data_cleaner = DataCleaner(config=data_cleaner_config)\n",
    "    data_cleaner.convert_examples_to_features()\n",
    "        \n",
    "except Exception as e:\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
